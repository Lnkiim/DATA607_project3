---
title: "Project 3"
author: "Team DAREZ"
date: "10/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# DeleteMe Notes

Delete these prior to posting.
Zach and Rachel will reorder sections and make sure the heading levles are correct once we have all the code in the same RMD. 

```{r include=FALSE, message=FALSE}
# This is formatted from a previous project. That example project can be found through the link:
# https://rpubs.com/deepakmongia/Data-607-Project-3
```


## Introduction

Example paragraph:
An October 2012 Harvard Business Review article called Data Scientist the “sexiest job of the 21st century.” In 21st-century time, 2012 is eons ago. Glassdoor, the popular career and company review site, has named Data Scientist the best job in America for three years running. We hear tales of huge starting salaries and high demand for talent. Still, for a number of reasons, we’re cautious.

### Setup 

How did we gather the data? What tools did we use? Examplain our setup for the analysis

```{r}
# For Web-scrap of Indeed
library(rvest)
library(readr)
library(tidyverse)
library(DT)
library(xml2)

# For graphing and nice tables
library("ggplot2")
library("rcartocolor")
library("kableExtra")

```


## Analysis

What are you going to look for in the data (besides just skills)? How are you going to get there? What cleaning was necessary?

How is our review relevant to data science today? Why did we choose these examples?

### Kaggle Survey 

In this first one, we could review the lit available from forums and publishers like Kaggle, Medium, Towards Data Science, Reddit, etc... then report on it.

### Indeed Scrape

Here, we could perform a small web-scraping study of a popular job site like Indeed and review the data for confirmation or denial of example 1.

Questions might include: 
  * What was the search for?  
    - i.e. title of data science
  * How many jobs in this scrape?
  * Where are they located?
  * What skills are required/preferred?
    - Are there any repeats?
    - Are most soft or hard skills?
  * What is the minimum experience?
  * What company is hiring the most?
  * How frequently are new jobs posted?
  * What is the average salary? 

Initial Run:

```{r echo=FALSE, eval = FALSE}
url <- 'https://www.indeed.com/jobs?q=data%20scientist&l&vjk=00ba1a22ba67ffd2'
webpage <- read_html(url)
job_data_html <- html_nodes(webpage,'.jobtitle , #sja0 b') 
job_data <- html_text(job_data_html)
head(job_data)
```

Found this string to be the most effective at parsing into usable form.

```{r, eval = FALSE}
str_extract(job_data, "(\\w+.+)+")
```

Repeat process for salary. Might be best to have it as a separate analysis without it being attached to jobs directly. It requires a lot of cleaning and figuring out how to categorize the ranges to understand the distribution. 

```{r, eval = FALSE}
url <- 'https://www.indeed.com/jobs?q=data%20scientist&l&vjk=00ba1a22ba67ffd2'
webpage <- read_html(url)
sal_data_html <- html_nodes(webpage,'.salaryText') 
sal_data <- html_text(sal_data_html)
head(sal_data)
```

Cleaning begins with:

```{r, eval = FALSE}
# Extracting the ranges provided as characters
salary_data <- str_extract(job_data, "(\\w+.+)+") 
# Removing the hourly rates - compare apples-apples
salary_data <- str_remove_all(salary_data, "\\d+ an hour")
# Remove the label " a year" since all shoudl be 
salary_data <- str_remove_all(salary_data, " a year")
# Remove the dollar sign
salary_data <- str_remove_all(salary_data,"\\$")
# Anticipating duplcicates - testing a solution
salary_data <- as.data.frame(salary_data)
salary_data[2,1] <- ("70,000 - $90,000")
salary_data %>% 
  mutate(salary = as.factor(job_data)) %>%
  count(salary)


```

Will need to run with more data to determine if effective. 

```{r message=FALSE, include=FALSE, eval = FALSE}
     ############################
    #      Do Not Run!         #
   #    See Data In Repo      #
  #     Labeled *Indeed*     #
 #        For Results       #
############################

empties <- data.frame(title=character(),
                    date=character(),
                  company=character(), 
                 salary=character(),
                 maxsal=character(),
                 minsal=character(),
                 location=character(), 
                 summary=character(), 
                 links=character(),
                 stringsAsFactors=FALSE) 
for (i in seq(0, 800, 10)){
  url_ds <- paste0('https://www.indeed.com/jobs?q=data%20scientist&l&vjk=dd25f8809ed80778',i)
  var <- read_html(url_ds)
  Sys.sleep(3)
title <-  var %>% 
      html_nodes('.jobtitle, #sja0 b') %>%
    html_text() %>%
      str_extract("(\\w+.+)+") 
date <-  var %>% 
      html_nodes('.date') %>%
    html_text() %>%
      str_extract("\\d+|Just posted|Today") 
company <- var %>% 
      html_nodes('.company') %>%
    html_text() %>%
      str_extract("(\\w+).+") 
job_data <- var %>%
    html_nodes('.salaryText') %>%
    html_text()
      salary <- str_remove_all(job_data, "\\d+.+ an hour| a year|\\$")
      maxsal <- str_extract(job_data, "- \\$\\d+,\\d+ ") %>%
       str_remove_all("- \\$| ")
      minsal <- str_extract(job_data, "\\$\\d+,\\d+ ") %>%
    str_remove_all(" |\\$")
location <- var %>%
        html_nodes('.location') %>%
      html_text() %>%
        str_extract("(\\w+.)+,.[A-Z]{2}")   
summary <- var %>%
        html_nodes('.summary') %>%
      html_text() %>%
        str_extract(".+")
links <- var %>%
        html_nodes('.jobtitle .turnstileLink, a.jobtitle') %>%
      html_attr('href') 
        link <- paste0("https://www.indeed.com",link)
        
Indeed <- rbind(empties, as.data.frame(cbind(title,
                                                  date,
                                                  company,
                                                  salary,
                                                  maxsal,
                                                  minsal,
                                                  location,
                                                  summary,
                                                  links
                                                  )))
}
```

Creates a data frame of the publicly posted jobs on Indeed at the time of the scrape called *Indeed*. It contains several attributes of the posted jobs, namely, the job title, the company name or employer,  location, job summary, and a link to the job page. 

## Indeed Analysis

The code above results in the CSV stored in our Github repository, which I'll read into a dataframe. 

```{r}
indeed_df <- read.csv(url("https://raw.githubusercontent.com/Lnkiim/DATA607_project3/main/Indeed.csv"), stringsAsFactors=FALSE)
indeed_df$minsal <- str_remove(indeed_df$minsal, "[,]")
indeed_df$minsal <- as.numeric(as.character(indeed_df$minsal))
indeed_df$maxsal <- str_remove(indeed_df$maxsal, "[,]")
indeed_df$maxsal <- as.numeric(as.character(indeed_df$maxsal))
glimpse(indeed_df)
```

Our Indeed scrape provides information on job posting locations, companies, and salaries. First, we can see that by far the greatest number of job posting at the time of this scrape were in Washington, DC and Lexington Park, MD (and NA) with over 100 in each location. 

```{r, message=FALSE}

indeed_toploc <- count(indeed_df, location, sort=TRUE)
indeed_toploc <- (top_n(indeed_toploc, 20))
indeed_toploc <- indeed_toploc %>%
  filter(rank(desc(n))<=50)

kable(indeed_toploc, format = "markdown")

indeed_toploc$location <- factor(indeed_toploc$location, levels = indeed_toploc$location[order(indeed_toploc$n)])

ggplot(indeed_toploc, aes(x = n, y = location, fill = n)) +
  geom_bar(stat="identity", fill="#53baa3") +
  labs(title="Job Listings for Top 20 Locations") +
  xlab('# of Listings') +
  ylab('Locations')
   

```

Taking a closer look, we see that for DC the postings primarily come from the CIA and the US Dept of the Treasury. All of the Lexington Park, MD postings are from the KBR. This is just a point-in-time snapshot of the job listings at the time of the scrape (during a strange economic time as well) - but it seems fair to conclude that in general the largest quantity of 'Data Science' jobs are available on the East Coast - one could say that proximity to a urban center is a desireable attribute for a data scientist, as that is where the work seems to be!

```{r}
indeed_dc <- subset(indeed_df, (location == "Washington, DC"))
indeed_topdc <- count(indeed_dc, company, sort=TRUE)
kable(top_n(indeed_topdc, 15), format = "markdown")

indeed_lex <- subset(indeed_df, (location == "Lexington Park, MD"))
indeed_toplex <- count(indeed_lex, company, sort=TRUE)
kable(top_n(indeed_toplex, 15), format = "markdown")
```


Finally, looking at the salary ranges, where they were avaiable, we see that the mean minimum on the salary range is $83,578 and the mean maximum is $139,151. Further, the mean range of the salary is $55,623 - which is also plotted. While there are certainly many missing values, it's interesting to see the variation in the salary range offered from this Indeed scrape.

```{r}

mean(indeed_df$minsal, na.rm=TRUE)
mean(indeed_df$maxsal, na.rm=TRUE)

indeed_df <- indeed_df %>%
  mutate(indeed_df, sal_range = maxsal - minsal)

mean(indeed_df$sal_range, na.rm=TRUE)

ggplot(indeed_df, aes(x = factor(sal_range), fill = sal_range)) +
  geom_bar(fill="#53baa3") +
  coord_flip() +  
  labs(title="Frequency of Salary Ranges") +
  xlab('Range (max-min)') +
  ylab('Frequency') 


```






### Glassdoor Text Analysis

This might be the best spot to demonstrate the results of a survey or another research example. 

## Conclusion

Which are the most valued data science skills? How do we know? Did we learn anything different than what the articles/literature said?

## Sources

Where did the data come from? Who analyzed each part of the data?
